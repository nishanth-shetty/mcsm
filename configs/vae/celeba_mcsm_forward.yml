training:
  batch_size: 128
  n_epochs: 3000
  n_iters: 100000
  algo: "mcsm"
  method: "forward"
  eps: 0.001

data:
  dataset: "CELEBA"
  image_size: 64
  channels: 3

model:
  n_particles: 1
  z_dim: 32
  eps_dim: 32
  nef: 64
  ndf: 64

optim:
  weight_decay: 0.000
  optimizer: "RMSProp"
  lr: 0.0001
